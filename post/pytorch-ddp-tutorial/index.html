<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Pytorch DDP Tutorial | Jiali Cui</title>
<link rel="shortcut icon" href="https://jialicui24.github.io/favicon.ico?v=1657039825860">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://jialicui24.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Pytorch DDP Tutorial | Jiali Cui - Atom Feed" href="https://jialicui24.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="One Machine Multiple GPUs (Code)
Import DDP Packages.
import torch.distributed as dist
import torch
import torch.multipr..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://jialicui24.github.io">
  <img class="avatar" src="https://jialicui24.github.io/images/avatar.png?v=1657039825860" alt="">
  </a>
  <h1 class="site-title">
    Jiali Cui
  </h1>
  <p class="site-description">
    Be Humble
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          Home
        </a>
      
    
      
        <a href="/archives" class="menu">
          Archives
        </a>
      
    
      
        <a href="/tags" class="menu">
          Tags
        </a>
      
    
      
        <a href="/post/about" class="menu">
          About Me
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Pytorch DDP Tutorial
            </h2>
            <div class="post-info">
              <span>
                2022-07-05
              </span>
              <span>
                4 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <h3 id="one-machine-multiple-gpus-code">One Machine Multiple GPUs (Code)</h3>
<h4 id="import-ddp-packages">Import DDP Packages.</h4>
<pre><code class="language-ruby">import torch.distributed as dist
import torch
import torch.multiprocessing as mp
</code></pre>
<h4 id="main-entrance">Main Entrance</h4>
<pre><code class="language-ruby">def main(local_rank, nprocs, args):
    # ==============================Prepare DDP ================================
    args.local_rank = local_rank
    init_seeds(args.seed)
    os.environ['MASTER_ADDR'] = args.master_address
    os.environ['MASTER_PORT'] = args.master_port
    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend='nccl', init_method='env://', world_size=nprocs, rank=local_rank)
    
    model, opt = build_model(args, local_rank)
    train_queue, _, _ = build_loaders(args)
    train(model, opt, train_queue, local_rank, nprocs, args)
    dist.destroy_process_group()
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='PyTorch Training')
    # Other hyper-params added
    # ================= DDP ===================
    parser.add_argument('--local_rank', default=-1, type=int, help='node rank for distributed')
    parser.add_argument('--seed', default=0, type=int, help='seed for initializing. ')
    parser.add_argument('--master_address', type=str, default='127.0.0.1', help='master addr')
    parser.add_argument('--master_port', type=str, default='6020', help='port for master')
    parser.add_argument('--nprocs', type=int, default=4, help='number of gpus')
    args = parser.parse_args()
    mp.spawn(main, nprocs=args.nprocs, args=(args.nprocs, args))
</code></pre>
<p><strong>local_rank</strong>: will set up automatically by torch.multiprocessing, where local_rank 0 is assumed to be the master one taking care of the logging jobs (print, save, ...).<br>
<strong>master_address</strong> and <strong>master_port</strong>: the address used for communication between GPUs. Make sure the address is not used.<br>
<strong>nprocs</strong>: The numbers of GPUs.</p>
<h4 id="build-model-and-loaders">Build Model and Loaders</h4>
<pre><code class="language-ruby">def average_params(params, is_distributed):
    &quot;&quot;&quot; parameter averaging. &quot;&quot;&quot;
    if is_distributed:
        size = float(dist.get_world_size())
        for param in params:
            dist.all_reduce(param.data, op=dist.ReduceOp.SUM)
            param.data /= size

def build_model(args, local_rank)

	model = Model(args).cuda(local_rank)
	
	dist.barrier() # wait
	# make sure the initialized params are the same
	average_params(model.parameters(), is_distributed=True) 
	
	# Adam for example
	opt = torch.optim.Adam(model.parameters(), lr=args.lr)
	
	return model, opt

def build_loaders(args)
	# Your DataSet train_data, valid_data
	
    train_sampler, valid_sampler = None, None
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)
        valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_data)
        
    train_queue = torch.utils.data.DataLoader(
        train_data, batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler, pin_memory=True, num_workers=8, drop_last=True)# 8

    valid_queue = torch.utils.data.DataLoader(
        valid_data, batch_size=args.batch_size,
        shuffle=(valid_sampler is None),
        sampler=valid_sampler, pin_memory=True, num_workers=1, drop_last=False) # 1

    return train_queue, valid_queue, num_classes
</code></pre>
<p><strong>averge_params</strong>: used to make sure the model located at different GPU is initialized the same.<br>
<strong>sampler</strong>: can split the dataset into (len(dataset) / #GPUs) sets and allocate them for GPUs. For example, 10000 images and 100 batch size, you will have 100 batchs for training with one GPU. Using four GPUs and sampler, only 25 batchs are for training on each GPU then. Speed up 4 times for training.</p>
<h4 id="training">Training</h4>
<pre><code class="language-ruby">def average_gradients(params, is_distributed): # syn grads between GPUs
    &quot;&quot;&quot; Gradient averaging. &quot;&quot;&quot;
    if is_distributed:
        size = float(dist.get_world_size())
        for param in params:
            if param.requires_grad:
                dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
                param.grad.data /= size
                
def train(model, opt, train_queue, local_rank, nprocs, args):
    global_step = 0
    for ep in range(args.epochs):
	    train_queue.sampler.set_epoch(global_step) # randomize training data queue
        for b, x in enumerate(train_queue):
            global_step += 1
            x = x[0] if len(x) &gt; 1 else x
		    x = x.cuda()
			model.train()
			# Loss 
			model.zero_grad()
			loss = model(x)
			loss.backward()
			average_gradients(model.parameters(), args.distributed)
            opt.step()
			
			model.eval()
			# Logging
			if local_rank == 0:
				print(logging)
				save_img(model)
			# saving
			if local_rank == 0 and save_condition:
				save(model, opt)

			# testing
			if local_rank == 0 and test_condition:
				test(model)
			dist_barrier() # wait master testing
	return
</code></pre>
<p><strong>average_gradients</strong>: make sure the updated parameters are the same.<br>
<strong>dist.barrier()</strong>: Again, it is for waiting and starting together.</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#one-machine-multiple-gpus-code">One Machine Multiple GPUs (Code)</a>
<ul>
<li><a href="#import-ddp-packages">Import DDP Packages.</a></li>
<li><a href="#main-entrance">Main Entrance</a></li>
<li><a href="#build-model-and-loaders">Build Model and Loaders</a></li>
<li><a href="#training">Training</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://jialicui24.github.io/post/learning-energy-based-models-in-highdimensional-spaces-with-multi-scale-denoising-score-matching/">
              <h3 class="post-title">
                Recent Read
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://jialicui24.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
